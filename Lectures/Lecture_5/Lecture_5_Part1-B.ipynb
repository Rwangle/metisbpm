{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.4.2 Advanced Manipulation of Pandas - Merge/Join/Concatenate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview:__\n",
    "- Merging, Joining, and Concatenating using Pandas Dataframes are very useful, however, advanced for the beginner programmer\n",
    "- However, the does not mean we can't explore the basics of what merging, joining, and concatenating is and how we can do it in Pandas \n",
    "- The full documentation for Merge, Join, and Concatenate can be found [here](http://pandas.pydata.org/pandas-docs/stable/merging.html)\n",
    "- All 3 methods below, in their own way, describe the act of combining together Series and DataFrame objects with some logic to handle the indexes of the objects you are combining:\n",
    "> 1. __Concatenating Objects:__ Concatenating objects in Pandas is done using the `concat` or the `append` function and you can think of this as \"gluing together\" multiple DataFrames or Series \n",
    "> 2. __Merging Objects:__ Merging objects in Pandas is done using the `merge` function and replicates [SQL Joins](https://en.wikipedia.org/wiki/Join_(SQL))\n",
    "> 3. __Joining Objects:__ Joining objects in Pandas is done using the `join` function and can be used for combining the columns of two potentially differently-indexed DataFrames into a single result DataFrame\n",
    "\n",
    "__Helpful Points:__\n",
    "1. We actually have already concatenated DataFrames above when we added a new row to an existing DataFrame \n",
    "2. Remember that indexes are very important to DataFrames and most of the \"logic\" that these methods employ revolve around what to do with DataFrames/Series that have different indexes\n",
    "3. SQL and Databases is definitely not required knowledge for this course, but further reading on Data Models (for your interest only) can be found [here](https://en.wikipedia.org/wiki/One-to-one_(data_model)) for __One-to-One Relationships__ and [here](https://en.wikipedia.org/wiki/Many-to-many_(data_model)) for __Many-to-Many Relationships__\n",
    "\n",
    "__Practice:__ Examples of Advanced Manipulation of Pandas - Merge/Join/Concatenate in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 (Concatenating Objects):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize 3 data frames to concatenate with same column names and index labels \n",
    "df1 = pd.DataFrame({'A': ['A0', 'A1'],\n",
    "                    'B': ['B0', 'B1']},\n",
    "                     index=[0, 1])\n",
    " \n",
    "df2 = pd.DataFrame({'A': ['A2', 'A3'],\n",
    "                    'B': ['B2', 'B3']},\n",
    "                     index=[2, 3])\n",
    "\n",
    "df3 = pd.DataFrame({'B': ['B2', 'B7'],\n",
    "                    'F': ['F2', 'F3']},\n",
    "                     index=[1, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.1 (Concatenating 2 DataFrames with Different Indexes):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.1.1 (Using Concat):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([df1, df2]) # concatenate one dataframe on another along rows \n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.1.2 (Using Append):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_result = df1.append(df2)\n",
    "a_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.2 (Concatenating 2 DataFrames with Same Indexes):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.2.1 (Using Concat):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1 (finds unique columns, duplicates rows)\n",
    "result = pd.concat([df1, df3], axis = 0) # concatenate one dataframe on another along rows\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2 (finds unique rows, duplicates columns)\n",
    "result_1 = pd.concat([df1, df3], axis = 1) # concatenate one dataframe on another along columns\n",
    "result_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 3 (uses only index from df1)\n",
    "result_2 = pd.concat([df1, df3], axis = 1, join_axes=[df1.index]) # concatenate one dataframe on another along columns\n",
    "result_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 4 (ignores and resets index)\n",
    "result_3 = pd.concat([df1, df3], ignore_index = True) # same as option 1 but with reset index \n",
    "result_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.2.2 (Using Append):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_result_1 = df1.append([df3]) # same as option 1 above\n",
    "a_result_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a_result_2 = df1.append([df3], ignore_index = True) # same as option 4 above\n",
    "a_result_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 (Merging Objects):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_1 = {\n",
    "        'department_id': ['1', '2', '3'],\n",
    "        'first_name': ['Gordon', 'Roberto', 'Jerod'], \n",
    "        'last_name': ['Dri', 'Reif', 'Rubalcava']}\n",
    "df_a = pd.DataFrame(raw_data_1, columns = ['department_id', 'first_name', 'last_name'])\n",
    "\n",
    "raw_data_2 = {\n",
    "        'department_id': ['2', '3', '4'],\n",
    "        'first_name': ['Jason', 'Zach', 'Paul'], \n",
    "        'last_name': ['Moss', 'Miller', 'Trowbridge']}\n",
    "df_b = pd.DataFrame(raw_data_2, columns = ['department_id', 'first_name', 'last_name'])\n",
    "\n",
    "raw_data_3 = {\n",
    "        'department_id': ['1', '2', '3', '4'],\n",
    "        'manager_id': ['12', '24', '13', '4']}\n",
    "df_c = pd.DataFrame(raw_data_3, columns = ['department_id', 'manager_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.1 (Merge DataFrames - Method 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.merge(pd.concat([df_a, df_b]), df_c, on = \"department_id\") # concatenate 2 employee df and then merge with department info df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.merge(pd.concat([df_a, df_b]), df_c, left_on = \"department_id\", right_on = \"department_id\") # same as above "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.2 (Merge DataFrames - Method 2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.merge(df_a, df_b, right_index = True, left_index = True) # merge based on indexes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.3 (Merge DataFrames - Method 3):\n",
    "\n",
    "__[Outer Join](https://blog.codinghorror.com/a-visual-explanation-of-sql-joins/):__ “Full outer join produces the set of all records in Table A and Table B, with matching records from both sides where available. If there is no match, the missing side will contain null.”\n",
    "\n",
    "- Don't worry about the terminology and definition, this is just for your reference. Just keep this method in your \"toolkit\" in the event that one of your use cases demands that your result is as appears below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.merge(df_a, df_b, on='department_id', how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.4 (Merge DataFrames - Method 4):\n",
    "\n",
    "__[Inner Join](https://blog.codinghorror.com/a-visual-explanation-of-sql-joins/):__ “Inner join produces only the set of records that match in both Table A and Table B.”\n",
    "\n",
    "- Don't worry about the terminology and definition, this is just for your reference. Just keep this method in your \"toolkit\" in the event that one of your use cases demands that your result is as appears below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.merge(df_a, df_b, on='department_id', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.5 (Merge DataFrames - Method 5):\n",
    "\n",
    "__[Left Join](https://blog.codinghorror.com/a-visual-explanation-of-sql-joins/):__ \"Left outer join produces a complete set of records from Table A, with the matching records (where available) in Table B. If there is no match, the right side will contain null.”\n",
    "\n",
    "- Don't worry about the terminology and definition, this is just for your reference. Just keep this method in your \"toolkit\" in the event that one of your use cases demands that your result is as appears below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.merge(df_a, df_b, on='department_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.6 (Merge DataFrames - Method 6):\n",
    "\n",
    "__[Right Join](https://en.wikipedia.org/wiki/Relational_algebra#Right_outer_join_(%E2%9F%96):__ \"Right outer join produces a complete set of records from Table B, with the matching records (where available) in Table A. If there is no match, the right side will contain null.”\n",
    "\n",
    "- Don't worry about the terminology and definition, this is just for your reference. Just keep this method in your \"toolkit\" in the event that one of your use cases demands that your result is as appears below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.merge(df_a, df_b, on='department_id', how='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 (Joining DataFrames):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_1 = {\n",
    "        'first_name': ['Gordon', 'Roberto', 'Jerod'],\n",
    "        'last_name': ['Dri', 'Reif', 'Rubalcava']}\n",
    "df_a = pd.DataFrame(raw_data_1, index = ['employee_1', 'employee_2', 'employee_3'])\n",
    "\n",
    "raw_data_1 = {\n",
    "        'department_id': ['1', '2', '2'],\n",
    "        'team_id': ['3', '3', '3']}\n",
    "df_b = pd.DataFrame(raw_data_1, index = ['employee_1', 'employee_3', 'employee_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.1 (Method 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a.join(df_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_a.join(df_b, how = \"left\") # same as above (default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.2 (Method 2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a.join(df_b, how = \"right\") # right join (see aboe for definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.3 (Method 3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a.join(df_b, how = \"outer\") # outer join (see aboe for definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.4 (Method 4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_a.join(df_b, how = \"inner\") # inner join (see aboe for definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.5 (Method 5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a.join(df_b, lsuffix = \"_l\", rsuffix = \"_r\") # overlapping columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.6 (Method 6):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_1 = {\n",
    "        'first_name': ['Gordon', 'Roberto', 'Jerod'],\n",
    "        'last_name': ['Dri', 'Reif', 'Rubalcava'],\n",
    "        'employee_num': [\"employee_1\", \"employee_2\", \"employee_3\"]}\n",
    "df_a = pd.DataFrame(raw_data_1)\n",
    "\n",
    "raw_data_1 = {\n",
    "        'department_id': ['1', '2', '2'],\n",
    "        'team_id': ['3', '3', '3']}\n",
    "df_b = pd.DataFrame(raw_data_1, index = [\"employee_1\", \"employee_2\", \"employee_3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a.join(df_b, on = \"employee_num\") # join on index and column "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.4.3 Advanced Manipulation of Pandas - Reshaping and Pivot Tables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview:__\n",
    "- It is common as a Data Scientist to work with data that is stored in CSV files or databases in \"stacked\" format (i.e. information for multiple groups stacked on top of each other)\n",
    "- However, it is inconvenient to perform analysis on stacked data, and much easier to work with data that is grouped by category or some other variable\n",
    "- In Excel, the __[\"Pivot Table\"](https://support.office.com/en-us/article/create-a-pivottable-to-analyze-worksheet-data-a9a84538-bfe9-40a9-a8e9-f99134456576)__ offers a helpful way to \"unstack\" stacked data in a format that is conducive for analysis\n",
    "- Pandas has very similar methods for \"pivoting/reshaping\" data that can be found [here](http://pandas.pydata.org/pandas-docs/stable/reshaping.html):\n",
    "> 1. Reshaping by __Pivoting DataFrame Objects:__ This reshaping can be done using the `pivot` function. The primary purpose here is to make the `columns` the unique variables and the `index` the time frame or identifier\n",
    "> 2. Reshaping by __Melting DataFrame Objects:__ This reshaping can be done using the `melt` function. The primary purpose here is to make one or more columns identifier variables and the other columns measured variables which are \"unpivoted\" to the row axis\n",
    "> 3. Reshaping by __Pivot Tables:__ This reshaping can be done using the `pivot_table` function. The primary purpose here is to pivot data AND provide aggregation of numeric data \n",
    "\n",
    "__Helpful Points:__\n",
    "1. Most people coming into Data Science have some experince with Excel, so these operations may feel \"at home\" for them\n",
    "2. Note the difference between the `pivot` and `pivot_table` functions. The former is used for general purpose pivoting of DataFrames and the latter is used for pivoting with aggregation of numeric data \n",
    "\n",
    "__Practice:__ Examples of Advanced Manipulation of Pandas - Reshaping and Pivot Tables in Python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 (Reshaping by Pivoting DataFrame Objects):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "values = [{\"Day\":\"Monday\", \"Buyer\":\"Mom\", \"Type\":\"Fuel\", \"Amount\":45},\n",
    "         {\"Day\":\"Tuesday\", \"Buyer\":\"Dad\", \"Type\":\"Food\", \"Amount\":200},\n",
    "         {\"Day\":\"Tuesday\", \"Buyer\":\"Gordon\", \"Type\":\"Sports\", \"Amount\":100},\n",
    "         {\"Day\":\"Monday\", \"Buyer\":\"Victor\", \"Type\":\"Music\", \"Amount\":90}]\n",
    "expenses_df = pd.DataFrame(values, columns = [\"Day\", \"Buyer\", \"Type\", \"Amount\"])\n",
    "expenses_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.1 (Pivot by Day and Buyer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "expenses_df.pivot(index = \"Day\", columns = \"Buyer\", values = \"Amount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.2 (Pivot by Buyer and Type):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "expenses_df.pivot(index = \"Type\", columns = \"Buyer\", values = \"Amount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 (Reshaping by Melting DataFrame Objects):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.1 (Melt by Type):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset for only type \n",
    "expenses_df.melt(id_vars = [\"Day\", \"Buyer\"], value_vars = [\"Type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.2 (Melt by Amount):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset for only amount\n",
    "expenses_df.melt(id_vars = [\"Day\", \"Buyer\"], value_vars = [\"Amount\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 (Reshaping by Pivot Tables):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expenses_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.1 (Pivot Table 1 - Aggregate Amount by Buyer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(expenses_df, values = \"Amount\", index = \"Buyer\", aggfunc = np.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.2 (Pivot Table 1 - Aggregate Amount by Day):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(expenses_df, values = \"Amount\", index = \"Day\", aggfunc = np.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.4.4 Advanced Manipulation of Pandas - GroupBy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.4.4.1 Introduction to GroupBy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview:__\n",
    "- The GroupBy functionality draws from the common \"Split-Apply-Combine\" strategy which is summarized below:\n",
    "> 1. __Split:__  The split step involves splitting the data into groups based on some criteria\n",
    "> 2. __Apply:__ The apply step involves applying a function to each group independently. The functions applied in this step fall in 3 main categories:\n",
    ">> a. __Aggregation:__ The aggregation functions compute a summary statistic about each group (i.e. group sums, means, sizes, counts, etc.)<br>\n",
    ">> b. __Transformation:__ The transformation functions perform some group-specific computations on each group (i.e. standardize data within each group, fill NAs within groups, etc.)<br>\n",
    ">> c. __Filtration:__ The filtration functions discard some groups according to a group-wise computation (i.e. discard data that belongs to groups with only a few members, filter out data based on group sum or mean, etc.)<br>\n",
    "> 3. __Combine:__ The combine step involves combining the results into a data structure \n",
    "\n",
    "__Helpful Points:__\n",
    "1. The GroupBy functionality exists in the SQL command `GROUP BY` which you can read about it [here](https://dev.mysql.com/doc/refman/5.7/en/group-by-modifiers.html) and [here](https://docs.microsoft.com/en-us/sql/t-sql/queries/select-group-by-transact-sql) if you are interested \n",
    "2. Arguably the most common functionality for Data Scientists is \"GroupBy-Aggregation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.4.4.2 GroupBy in Pandas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview:__\n",
    "- The __[GroupBy](http://pandas.pydata.org/pandas-docs/stable/groupby.html)__ function in Pandas allows us to perform the \"split-apply-combine\" strategy in a very efficient manner\n",
    "- The procedure is performed in the following steps where the \"combine\" step is implicit in the \"apply\" step:\n",
    "\n",
    "> /1. __STEP 1 (SPLIT):__ Splitting is completed by creating a \"GroupBy\" object which basically just means separating out the data into constituent groups. GroupBy objects can be created in the following way: \n",
    ">> a. `grouped = obj.groupby(key)`<br>\n",
    ">> b. `grouped = obj.groupby(key, axis=1)`<br>\n",
    ">> c. `grouped = obj.groupby([key1, key2])`\n",
    ">> - Here, the `obj` is usually a Pandas `DataFrame` object and the `key` is most commonly one of the following:<br>\n",
    ">> > a. List or NumPy array<br> \n",
    ">> > b. Dictionary or Series<br>\n",
    ">> > c. Column name<br>\n",
    ">> - In practice, the \"group keys\" will refer to the labels of each group (i.e. \"A\" and \"B\" are the group keys for Group \"A\" and Group \"B\" and \"Group\" would be passed as the `key` argument into the `groupby` function) \n",
    "\n",
    "> /2. __STEP 2 (APPLY):__ Now that a GroupBy object has been created, we can perform various \"apply\" methods to the object such as:\n",
    ">> a. __Aggregation:__ Aggregation returns a new DataFrame and can be completed in this fashion: `grouped.agg` or `grouped.aggregate`<br>\n",
    ">> b. __Transformation:__ Transformation returns an object that is indexed the same as the one begin grouped and can be completed in this fashion: `grouped.transform`<br>\n",
    ">> c. __Filtration:__ Filtration returns a subset of the original object and can be completed in this fashion: `grouped.filter`<br>\n",
    ">> d. __Other:__ `grouped.count`, `grouped.cummax`, `grouped.cumsum`, `grouped.fillna`, `grouped.max`, etc. \n",
    "\n",
    "__Helpful Points:__\n",
    "1. GroupBy objects have a few useful attributes for analyzing the groups that were created:\n",
    "> 1. The `groups` attribute to view the groups that were created: `df.groupby(key).groups`\n",
    "> 2. The `describe` attribute to view information about the groups that were created: `df.groupby(key).describe()`\n",
    "> 3. The `ngroups` attribute to view the number of groups that were created: `df.groupby(key).ngroups`\n",
    "> 4. The `first` attribute to view the first few rows of the groups that were created: `df.groupby(key).first()`\n",
    "> 5. The `last` attribute to view the last few rows of the groups that were created: `df.groupby(key).last()`\n",
    "> 6. The `get_group` attribute to access one of the many groups that were created:`df.groupby(key).get_group()`\n",
    "2. Remember you can always see a complete list of all the attributes of an object by selecting `obj.<TAB>`\n",
    "\n",
    "__Practice:__ Examples of GroupBy in Pandas in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 (Step 1 - Split):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "department_df = pd.DataFrame({'Department' : ['Marketing', 'HR', 'Marketing', 'HR',\n",
    "                           'Marketing', 'HR', 'Marketing', 'Marketing'],\n",
    "                   'City' : ['Chicago', 'Chicago', 'Seattle', 'New York',\n",
    "                           'Seattle', 'Seattle', 'Chicago', 'New York'],\n",
    "                   'People' : [100, 20, 31, 300,\n",
    "                           1000, 300, 450, 5],\n",
    "                   'Revenue' :[200, 200, 31, 350,\n",
    "                           1200, 300, 4050, 50]}, columns = [\"Department\", \"City\", \"People\", \"Revenue\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "department_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.1 (Group by Department):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped_dpt = department_df.groupby('Department')\n",
    "grouped_dpt # groupby object "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.2 (Attributes of GroupBy Object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dpt.groups # groups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped_dpt.describe() # summary statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dpt.ngroups # number of groups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped_dpt.first() # first row of each group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dpt.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped_dpt.last() # last row of each group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped_dpt.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dpt.nth(2) # nth row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.3 (Group by City):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_city = department_df.groupby('City')\n",
    "grouped_city # groupby object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dpt[\"City\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dpt.get_group(\"HR\") # get one group "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 (Step 2 - Apply):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "department_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1 (Aggregation):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.1.1 (Aggregate by Sum):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dpt.aggregate(np.sum) # aggregate sum based on one group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dpt.agg(np.sum) # note: we can also just write agg instead of aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "department_df.groupby(\"Department\", as_index = False).sum() # if we want the group keys as a column and not index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new groupby object by department AND city\n",
    "grouped_dpt_city = department_df.groupby([\"Department\", \"City\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped_dpt_city.aggregate(np.sum) # aggregate sum based on two groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is actually an example of \"multiple keys\" since we have both \"Department\" and \"City\" as Keys and therefore the result is a __[MultiIndex](http://pandas.pydata.org/pandas-docs/stable/advanced.html#advanced-hierarchical) or Hierarchical Index__ which is not covered in this course. We can avoid this by making the group keys both into columns like we did above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "department_df.groupby([\"Department\", \"City\"], as_index = False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.1.2 (Aggregate by Mean):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dpt.aggregate(np.mean) # means based on one group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dpt_city.aggregate(np.mean) # means based on two groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.1.3 (Aggregate by Sizes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dpt.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dpt_city.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.1.4 (Aggregate by Multiple Functions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dpt.aggregate([np.sum, np.mean, np.std]) # this is similar to describe() above "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2 (Transformation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "department_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.2.1 (Transform to Sum of Groups):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "department_df[\"Revenue_Total\"] = grouped_dpt[\"Revenue\"].transform('sum') # transform revenue to be total departmental revenue\n",
    "department_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.2.2 (Transform to Z-Score of Groups):\n",
    "\n",
    "We can also use a manual function inside the transform argument. In this case, we are using a function that calculates the __[\"Z-Score\"](https://en.wikipedia.org/wiki/Standard_score)__ which is a standardized measure that is calculated by subtracting the mean from every value and dividing this by the standard deviation. We will see this in more detail in Lectures 11 and 12.  \n",
    "\n",
    "- Don't worry about the definition of Z-Score here. The point of this example is not to teach the Z-Score, but to see how we can apply a manual function to each group and transform the values\n",
    "- The most important thing to note here is that the function is being applied WITHIN groups only. See calculation for \"Standardized_Revenue\" for the Marketing Department in Chicago:\n",
    "\n",
    "Group Mean across all Marketing Departments: $1106.2$<br>\n",
    "Group Standard Deviation across all Marketing Departments: $1715.2$<br>\n",
    "Z-Score for Marketing-Chicago: $(200 - 1106.2)/1715.2 = -0.528$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore = lambda x: (x-x.mean()) / x.std()\n",
    "zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "department_df[\"Standardized_Revenue\"] = grouped_dpt[\"Revenue\"].transform(zscore) # transform revenue to be total departmental revenue\n",
    "department_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.2.3 (Transform to Range):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "department_df[\"Revenue_Range\"] = grouped_dpt[\"Revenue\"].transform('max') - grouped_dpt[\"Revenue\"].transform('min') \n",
    "department_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: NOT shown here is an application of replacing missing data with group mean values. Since the transform method is applied WITHIN groups, we can impute missing data with the group mean value that observation belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.3 (Filtration):\n",
    "\n",
    "Recall in Lecture 4 our discussion of the `map`, `reduce`, and `filter` functions that were used in conjunction with `lambda` functions. Here is no different except we are applying the function WITHIN groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.3.1 (Subset - 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only elements that belong to groups with a group sum greater than 100\n",
    "grouped_dpt[\"Revenue\"].filter(lambda x: x.sum() > 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns all the elements from the Marketing group since this was the only group that had a sum greater than 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.3.2 (Subset - 2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only elements that belong to groups that have less than 5 members\n",
    "grouped_dpt[\"Revenue\"].filter(lambda x: len(x) < 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns all the elements from the HR group since this was the only group that had less than 5 members "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.5 Operations on Pandas Data Structures:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.5.1 Operations on Elements, Rows, and Columns:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview:__\n",
    "- Pandas offers various ways to operate on Series and DataFrames in both a row/column fashion (i.e. vectorized) and an element-wise fashion\n",
    "- Operations can be broken down into the following two types:\n",
    "> 1. __Operations with Loops:__ There are 2 main types of loop in Pandas Data Structures:\n",
    ">> a. __Looping over Rows:__ To loop over rows, it is possible to use the functions __[`df.itertuples`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.itertuples.html)__ (which iterates over DataFrame rows as namedtuples of the values) or __[`df.iterrows`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html)__ (which iterates over (column, Series) pairs <br>\n",
    ">> b. __Looping over Columns:__ To loop over columns, it is possible to use the functions __[`df.iteritems`](http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.iteritems.html)__\n",
    "> 2. __Operations without Loops:__ There are 3 main types of functions that are used to perform operations without loops:\n",
    ">> a. __[`apply`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html):__ The `apply` function works on a row/column basis on `DataFrame` objects. The syntax is as follows: `df.apply(func, axis=0)`, where the `axis=0` is to apply function over each column and `axis=1` is to apply function over each row<br> \n",
    ">> b. __[`map`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html):__ The `map` function works on an elment-wise basis on `Series` objects. The syntax is as follows: `s.map(arg)` where `arg` can be a function, dictionary or series<br>\n",
    ">> c. __[`applymap`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.applymap.html):__ The `applymap` function works on an element-wise basis on `DataFrame` objects. The syntax is as follows: `df.applymap(func)`\n",
    "\n",
    "__Helpful Points:__\n",
    "1. Some users have expressed discontent and overall discouragement over using `df.iterrows` to iterate over rows of a DataFrame in favour of vectorized solutions (see Operations without Loops above) OR `df.itertuples`. You can read about some of `df.iterrows` performance concerns [here](https://stackoverflow.com/questions/24870953/does-iterrows-have-performance-issues) if you are interested \n",
    "2. In each case of the operations without loops, the function that may be entered in as an argument can be a manually made one using the `lambda` expression or a function that is supported by NumPy in the form of `np.func`\n",
    "\n",
    "__Practice:__ Examples of Operations on Elements, Rows, and Columns on Pandas Data Structures in Python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 (Operations with Loops):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df_small = pd.DataFrame(np.arange(10))\n",
    "my_df_large = pd.DataFrame(np.arange(100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df_small.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.1 (Looping over Rows - Iterrows):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for row in my_df_small.iterrows():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `iterrows()` iterates over DataFrame rows as (index, Series) pairs. For example:\n",
    "\n",
    "1. The first (index, Series) pairs is `(0, 0   0)` which refers to the `0` index (first row) and the Series object which is the column name (`0`) and the value `0`\n",
    "2. The seconds (index, Series) pairs is `(1, 0   0)` which refers to the `1` index (second row) and the Series object which is the column name (`0`) and the value `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit \n",
    "\n",
    "for row in my_df_large.iterrows():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.2 (Looping over Rows - Itertuples):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for row in my_df_small.itertuples():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `itertuples()` iterates over DataFrame rows as namedtuples. For example:\n",
    "\n",
    "1. The first namedtuple is `(Index=0, _1=0)` which refers to the tuple that contains the `Index=0` and `_1` which refers to the name of the column and the value in that column (`0`)\n",
    "2. The second namedtuple is `(Index=1, _1=1)` which refers to the tuple that contains the `Index=1` and `_1` which refers to the name of the column and the value in that column (`1`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "for row in my_df_large.itertuples():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this time result using `itertuples()` with the time result above using `iterrows()`. The mean time of `itertuples()` was 44.8 ms per loop compared to `iterrows()` which was over 3 seconds!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.3 (Looping over Columns - Iteritems):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df_small_T = my_df_small.T\n",
    "my_df_large_T = my_df_large.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df_small_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in my_df_small_T.iteritems():\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same format as `df.iterrows()` above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 (Operations without Loops):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "department_df_sub = department_df.loc[:, [\"People\", \"Revenue\"]]\n",
    "revenue = department_df[\"Revenue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "department_df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.1 (Operations without Loops - Apply):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.1.1 (Sum):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "department_df_sub.apply(np.sum, axis = 0) # apply function to each column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "department_df_sub.sum(axis = 0) # non-apply equivalent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "department_df_sub.apply(np.sum, axis = 1)# apply function to each row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "department_df_sub.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.1.2 (Range):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "department_df_sub.apply(lambda x: x.max() - x.min()) # apply function to each column "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.2 (Operations without Loops - Map):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue.map(lambda x: '%.2f' % x) # insert 2 decimal places for each element in the Series "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.3 (Operations without Loops - Applymap):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "department_df_sub.applymap(lambda x: '%.2f' % x) # insert 2 decimal places for each element in the Series "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.5.2 Operations to Handle Missing Data in Pandas Data Structures:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview:__\n",
    "- In Data Science, when working with real-world data, it is very common to encounter missing data that may be a result of data not available, data not recorded, or data not present for some reason\n",
    "- Since the purpose of this course is not teach tools for analyzing data, per se, we won't cover handling missing data in too much detail\n",
    "- However, as a beginner you should be aware of the possibile options that Pandas provides for storing missing data, finding missing data, inserting missing data, and cleaning/filling missing data which can be read about in detail [here](http://pandas.pydata.org/pandas-docs/stable/missing_data.html) and explained below:\n",
    "\n",
    "> 1. __Storing Missing Data:__ The value `NaN` is the default missing value marker, but sometimes `None` is used as well \n",
    "> 2. __Finding Missing Data:__ Pandas offers many convenient functions for testing if values are missing such as: `df.isna()`, `df.notna()`, `df.isnull()`, `df.notnull()` which all return a boolean same-sized object indicating if the values are missing. It is also possible to use these functions to count the number of missing values (i.e. `np.count_nonzero(df.isnull())`, `np.count_nonzero(np.any(df.isnull(), axis=)`)\n",
    "> 3. __Inserting Missing Data:__ It is possible to insert a missing value by setting values in Series or DataFrame objects as `None` or `np.nan`\n",
    "> 4. __Cleaning/Filling Missing Data:__ Pandas offers many convenient functions for filling missing values such as: `df.fillna(value)` and `df.dropna(axis)`\n",
    "\n",
    "__Helpful Points:__\n",
    "1. Note two `NaN` values do not compare equal (i.e. `np.nan == np.nan` is evaluated as `False`)\n",
    "\n",
    "__Practice:__ Examples of Operations to Handle Missing Data in Pandas Data Structures in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "na_df = df = pd.DataFrame(np.arange(15).reshape(5, 3), index=['a', 'c', 'e', 'f', 'h'],\n",
    "                          columns=['one', 'two', 'three'])\n",
    "na_df = na_df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])\n",
    "na_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 (Finding Missing Data):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.1 (Finding Missing Data - Method 1):\n",
    "\n",
    "Newer versions of pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_df.isna() # check elements that are missing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "na_df.notna() # check elements that are not missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.2 (Finding Missing Data - Method 2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_df.isnull() # check elements that are missing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "na_df.notnull() # check elements that are not missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.3 (Finding Missing Data - Method 3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(na_df.isnull()) # total number of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(np.any(na_df.isnull(), axis=0)) # total number of columns with missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.count_nonzero(np.any(na_df.isnull(), axis=1)) # total number of rows with missing values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 (Inserting Missing Data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "na_df[\"four\"] = \"Gordon\"\n",
    "na_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.1 (Inserting Missing Data into Int):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_df.loc[\"a\", \"two\"] = None # fills as NaN since replacing an int\n",
    "na_df.loc[\"a\", \"three\"] = np.nan # fills as NaN \n",
    "na_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.2 (Inserting Missing Data into Int):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_df.loc[\"a\", \"four\"] = np.nan # fills as None since replacing a str\n",
    "na_df.loc[\"b\", \"four\"] = None # fills as None\n",
    "na_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 (Cleaning/Filling Missing Data):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.1 (Filling NA with Value):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "na_df.fillna(\"NEW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.2 (Dropping Axis Labels with Missing Values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "na_df.dropna(axis=0) # remove rows that have NA values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.5.3 Operations to Handle Time Series Data in Pandas Data Structures:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview:__\n",
    "- As a Data Scientist, it is common to encounter data that takes place over time (i.e. sales data, financial data, etc.) which are commonly known as __[Time Series](https://en.wikipedia.org/wiki/Time_series)__ Data \n",
    "- Recall in out early discussions about the usefulness of Pandas and one of the major points we outlined which was Pandas' Time Series extensive capabilities which can be read about in detail [here](https://pandas.pydata.org/pandas-docs/stable/timeseries.html)\n",
    "- We will only briefly explore Pandas Time Series/Dates functionality here as they pertain to:\n",
    "> 1. Initiating objects of the following Date-Time types:\n",
    ">> a. `Timestamp` (single timestamp): created by `pd.to_datetime()`, `pd.Timestamp`<br>\n",
    ">> b. `DatetimeIndex` (index of `Timestamp`): created by `pd.datetime()`, `pd.date_range()`, `pd.DatetimeIndex`<br>\n",
    ">> c. `Period` (single time span): created by `pd.Period`<br>\n",
    ">> d. `PeriodIndex` (index of `Period`): created by `pd.period_range`, `pd.PeriodIndex`\n",
    "> 2. Common operations:\n",
    ">> a. `df.shift()`: Shifts index by desired number of periods<br> \n",
    ">> b. `df.resample()`: Resampling operations during frequency conversion (i.e. converting secondly data into 5-minutely data). It operates as time-based GroupBy (see above)<br>\n",
    ">> c. `df.rolling()`: Provides rolling window calculations \n",
    "\n",
    "__Helpful Points:__\n",
    "1. Pandas Time Series capabilities leverage the NumPy `datetime64` and `timedelta54` dtypes and you will find these as the type of the object after initializing a Timestamp, Period, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 (Initiating Objects):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.1 (Initiating Timestamp):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.1.1 (Using `pd.Timestamp()`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Timestamp(\"2018-04-23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Timestamp(2018, 4, 23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.1.2 (Using `pd.to_datetime()`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(\"2018/04/23\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.2 (Initiating DatetimeIndex):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.2.1 (Using `pd.date_range()`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_week = pd.date_range(\"2018/04/23\", periods = 7, freq = \"D\") # first week of class \n",
    "first_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_week_day_num = pd.Series(np.arange(1,8), index = first_week)\n",
    "first_week_day_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_first_week = first_week_day_num.asfreq(\"B\") # class only happens on business days \n",
    "converted_first_week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [here](http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases) for the possible time series frequencies that can be used and their Pandas Aliases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.3 (Initiating Period):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.3.1 (Using `pd.Period()`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Period(\"2018-04\") # inferred as Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.Period(\"2018-04-23\") # inferred as Day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.4 (Initiating PeriodIndex):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.4.1 (Using `pd.period_range()`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.period_range(start=\"2018-04-23\", end = \"May-31-2018\", freq = \"M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 (Common Operations):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.1 (Using Shift):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_first_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "converted_first_week.shift(2) # shift values by 2 periods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.2 (Using Resample):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = pd.date_range('4/23/2018', periods=100, freq='S') # range of 100 seconds\n",
    "ts = pd.Series(np.arange(len(rng)), index = rng) # fill in values of 0 - 100\n",
    "ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts.resample(\"Min\").sum() # 1 minute totals "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.3 (Using Rolling):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Counts': [10, 31, 42, 5, 1, np.nan]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.rolling(2).sum() # rolling sum with window length of 2 (sum of every 2 values) - ignored NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rolling(window = 2, min_periods = 1).sum() # rolling sum with window length of 2 (sum of every 2 values) - adds if there is at least 1 value in the window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
